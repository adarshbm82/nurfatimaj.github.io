[{"authors":["admin"],"categories":null,"content":"Welcome!\nI am a PhD student at the Department of Economics at the European University Institute.\nMy area of expertise is Applied Microeconometrics. My research is mainly focused on educational choices and their effects on labour market outcomes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nurfatimaj.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Welcome!\nI am a PhD student at the Department of Economics at the European University Institute.\nMy area of expertise is Applied Microeconometrics. My research is mainly focused on educational choices and their effects on labour market outcomes.","tags":null,"title":"Nurfatima Jandarova","type":"authors"},{"authors":["aichino"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"781e653cbbcf07ee9e996d12c5ae4bd3","permalink":"https://nurfatimaj.com/authors/aichino/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aichino/","section":"authors","summary":"","tags":null,"title":"Andrea Ichino","type":"authors"},{"authors":["aichino"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"e3825803958bcee7c7eaaa92750cb108","permalink":"https://nurfatimaj.com/authors/arustichini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/arustichini/","section":"authors","summary":"","tags":null,"title":"Aldo Rustichini","type":"authors"},{"authors":["aichino"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"df1e9faf8af7b263dc3477a583303413","permalink":"https://nurfatimaj.com/authors/gzanella/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gzanella/","section":"authors","summary":"","tags":null,"title":"Giulio Zanella","type":"authors"},{"authors":["jlreuter"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"6655caf5f630fc025a067b6ec176656c","permalink":"https://nurfatimaj.com/authors/jlreuter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jlreuter/","section":"authors","summary":"","tags":null,"title":"Johanna Reuter","type":"authors"},{"authors":null,"categories":["tech fun"],"content":"At the beginning of PhD I used to have at most one project at a time. Keeping track of things was easy. Along the way, I accumulated a portfolio of projects. For each of them I write tons of codes (partly because my programmer sister has taught me that having one big file with everything is bad). One day I found myself looking at my own code of three months earlier and spending half a day to retrace its logic. There were dozens of errors before I was able to run everything smoothly without forgetting some annoying little detail.\nThen it became clear to me that I need to keep a proper documentation. Hopefully, it is also helpful when one day the papers get published. Be it couple of files, I could have done it all manually. Or maybe not\u0026hellip; Just the thought that I would need to keep track of twice as many documents sends me to shiver. Search for some ready solution did not yield much results: either my target programming languages are too \u0026ldquo;simple\u0026rdquo;, or the output is not what I was looking for.\nAfter a little thinking it struck me that automatic documentation is nothing more than a text parsing exercise. So I set out to write my own script. It searches for all new or recently modified code files in my project folder, parses the text and pastes it to a markdown file in my documentation subfolder. In this project I rely a lot on git version control and remote repository that keeps all important code files.\nTechnical bits Code structure The benefit of having an automatic documentation generation script is that I can keep all documentation-relevant information inside the code file. Then keeping documentation up-to-date is only a matter of updating comments inside the code file.\nSpecial comment Not all comments are meant to be inside a documentation file. Detecting the relevant code blocks is solved by including a special comment block (#' or *' for Stata) which I have borrowed from R. So all lines that start with the special sign will be part of the documentation file.\nParsing and outputing After detecting the special comment block, the script is ridiculously simple. Basic idea is to strip away the special comment sign and simply copy the rest of the text into a markdown file. Of course, for this to work, the documentation comment block should follow a simple markdown syntax.\nMy documentation script applies additional rules to certain lines, which, again borrowing from R, are identified with special tags (for example, @title for document title). These rules work by identifying if a line starts with one of the special tags, stripping the tag away and outputing the rest of the text in a desired way. For example, when a line starts with @title, I insert to the title text hyperlink to the actual code file. The linking is possible because I am using git version control with a remote repository.\nTo a more technically inclined reader, all that my script does here is continuously apply sed to the extracted lines and echo the output into a markdown file.\nInput The script works both with single code file and with a project folder that contains the code files. That is, it is really easy to mass generate documentation files.\nHowever, not all the codes in the folder need documentation; for example, a script from workshop I use as a reference. My script relies on git version control in deciding which files are relevant: if it is controlled by git, then it is worth a documentation.\nPublishing Since I am using git and remote repository, I am also using wiki submodule to publish the documentation files. While GitHub requires public repository for wiki functionality to work, Bitbucket allows it on private repositories too. The path to the documentation files inside wiki follows is similar to the code file path inside project directory.\nIf I have time in the future, \u0026hellip; \u0026hellip; I would like to make the script a bit more flexible. Currently, my script parses and outputs documentation information line by line. Therefore, it can neither reorder sections nor augment with information found later under the same section heading. The output will look exactly the same way as the comment block. So, a nice extension would have been to extract all section-relevant data into a separate array and then output everything together. This would allow special comments organized like this\n#' @section1 #' comment 1 #' #' @section2 #' comment 1 #' #' @section1 #' comment 2  to appear correctly in the documentation file: two comments under section 1 and one comment under section 2.\nSummary Now, I can just click on respective wiki post and see all relevant information: usage, argument types and accepted values, description, input and output files. No need to traverse the code to make sense of it. Just work on the actual task.\n","date":1566038022,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571275533,"objectID":"1c238270ce3f9bd77542190e87b2151b","permalink":"https://nurfatimaj.com/post/tech-fun/automatic-documentation/","publishdate":"2019-08-22T20:00:00+06:00","relpermalink":"/post/tech-fun/automatic-documentation/","section":"post","summary":"At the beginning of PhD I used to have at most one project at a time. Keeping track of things was easy. Along the way, I accumulated a portfolio of projects. For each of them I write tons of codes (partly because my programmer sister has taught me that having one big file with everything is bad). One day I found myself looking at my own code of three months earlier and spending half a day to retrace its logic. There were dozens of errors before I was able to run everything smoothly without forgetting some annoying little detail.\n","tags":["software","documentation","git","bash script","markdown","text parsing"],"title":"Automatic documentation","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571299166,"objectID":"07e82b29ab4ada67b89956e1eb6ed8e1","permalink":"https://nurfatimaj.com/print/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/print/","section":"","summary":"","tags":null,"title":"CV","type":"widget_page"},{"authors":["Nurfatima Jandarova","Andrea Ichino","Aldo Rustichini","Johanna Reuter","Giulio Zanella"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571304500,"objectID":"ece28276351dd5408568671f8a778271","permalink":"https://nurfatimaj.com/publication/work-in-progress/uni-sorting-genetics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/work-in-progress/uni-sorting-genetics/","section":"publication","summary":"In the UK the number of students with a university degree has increased considerably between 1950 and 1970. Out of 100 high-school students, only about 3 went to university at the beginning of this period while about 8 did so in 1970 and about 19 in 1990. We know that, with few exceptions, only rich children had the possibility to attend college in 1950, independently of their skills. What we do not know is the answer to this question: did the expansion of enrollment after 1950 succeed in giving poor but smart children the opportunity to access a university? The objective of this research project is to answer this question. It is an important question because if, the answer is no, politicians need to think of better policies to achieve the goal of improving the opportunities of smart but poor children.","tags":null,"title":"Genetic Endowment and Sorting into University Education","type":"publication"},{"authors":["Nurfatima Jandarova","Johanna Reuter"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571304500,"objectID":"2369091e170bf255ee956ebcbb898e41","permalink":"https://nurfatimaj.com/publication/work-in-progress/measurement-edu-uk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/work-in-progress/measurement-edu-uk/","section":"publication","summary":"","tags":null,"title":"Improving UK university education rate statistic using multiple imputation","type":"publication"},{"authors":["Nurfatima Jandarova"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571275533,"objectID":"2195c6d4d67be36d5d70fbcf0e75c660","permalink":"https://nurfatimaj.com/publication/work-in-progress/edu-unemp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/work-in-progress/edu-unemp/","section":"publication","summary":"This project studies the effect of labour market shocks on educational choices and whether these educational choices translate into more favourable outcomes later in life. For causal identification I exploit local employment shocks at the time of finishing compulsory schooling using the rich dataset on the population of university students in the UK from 1972 to 1993.","tags":null,"title":"Unemployment and educational choices","type":"publication"}]