[{"authors":["admin"],"categories":null,"content":"Welcome!\nI am a PhD student at the Department of Economics at the European University Institute.\nMy area of expertise is Applied Microeconometrics. My research is mainly focused on educational choices and their effects on labour market outcomes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571646665,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nurfatimaj.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Welcome!\nI am a PhD student at the Department of Economics at the European University Institute.\nMy area of expertise is Applied Microeconometrics. My research is mainly focused on educational choices and their effects on labour market outcomes.","tags":null,"title":"Nurfatima Jandarova","type":"authors"},{"authors":["aichino"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"781e653cbbcf07ee9e996d12c5ae4bd3","permalink":"https://nurfatimaj.com/authors/aichino/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aichino/","section":"authors","summary":"","tags":null,"title":"Andrea Ichino","type":"authors"},{"authors":["aichino"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"e3825803958bcee7c7eaaa92750cb108","permalink":"https://nurfatimaj.com/authors/arustichini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/arustichini/","section":"authors","summary":"","tags":null,"title":"Aldo Rustichini","type":"authors"},{"authors":["aichino"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"df1e9faf8af7b263dc3477a583303413","permalink":"https://nurfatimaj.com/authors/gzanella/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gzanella/","section":"authors","summary":"","tags":null,"title":"Giulio Zanella","type":"authors"},{"authors":["jlreuter"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571304500,"objectID":"6655caf5f630fc025a067b6ec176656c","permalink":"https://nurfatimaj.com/authors/jlreuter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jlreuter/","section":"authors","summary":"","tags":null,"title":"Johanna Reuter","type":"authors"},{"authors":["Nurfatima Jandarova"],"categories":[],"content":"  Dataset source: github repository of the Department of Civil Protection\nSetup library(tidyverse) library(lubridate) library(sf) library(gganimate) data_file \u0026lt;- \u0026#39;~/Downloads/COVID-19/dati-regioni/dpc-covid19-ita-regioni.csv\u0026#39; shape_dsn \u0026lt;- \u0026#39;~/Downloads/italian-maps-shapefiles/italy-with-regions/\u0026#39; df \u0026lt;- read_csv(data_file) shape \u0026lt;- read_sf(dsn = shape_dsn) It is also helpful to reorder the regions based on north-south scale.\ndf \u0026lt;- df %\u0026gt;% mutate(distance = sqrt(lat^2 + long^2), regions = fct_reorder(denominazione_regione, desc(distance))) Since the hour information is useless, letâ€™s remove it\ndf \u0026lt;- df %\u0026gt;% mutate(date = date(data)) %\u0026gt;% select(-data) Now, I need to convert the dataset into simple features collection supplying the long and lat as point coordinates.\ndf_sf \u0026lt;- df %\u0026gt;% st_as_sf(coords = c(\u0026#39;long\u0026#39;, \u0026#39;lat\u0026#39;), crs = 4326, agr = \u0026#39;constant\u0026#39;) And some settings for the plots\ntheme_set(theme_minimal()) theme_update(legend.position = \u0026#39;bottom\u0026#39;) options(ggplot2.continuous.colour = \u0026#39;Spectral\u0026#39;) options(ggplot2.continuous.fill = \u0026#39;Spectral\u0026#39;) myColourPalette \u0026lt;- c(\u0026#39;#67001f\u0026#39;,\u0026#39;#b2182b\u0026#39;,\u0026#39;#d6604d\u0026#39;,\u0026#39;#f4a582\u0026#39;,\u0026#39;#fddbc7\u0026#39;,\u0026#39;#f7f7f7\u0026#39;,\u0026#39;#d1e5f0\u0026#39;,\u0026#39;#92c5de\u0026#39;,\u0026#39;#4393c3\u0026#39;,\u0026#39;#2166ac\u0026#39;, \u0026#39;#053061\u0026#39;, \u0026#39;white\u0026#39;, \u0026#39;#40004b\u0026#39;,\u0026#39;#762a83\u0026#39;,\u0026#39;#9970ab\u0026#39;,\u0026#39;#c2a5cf\u0026#39;,\u0026#39;#e7d4e8\u0026#39;,\u0026#39;#f7f7f7\u0026#39;,\u0026#39;#d9f0d3\u0026#39;,\u0026#39;#a6dba0\u0026#39;, \u0026#39;#5aae61\u0026#39;,\u0026#39;#1b7837\u0026#39;,\u0026#39;#00441b\u0026#39;) #from colorbrewer2.org  Plots Testing activity and new cases The capacity to identify confirm cases is limited by the testing capacity; therefore, it helps to look at both testing and positive cases simultaneously, whether in absolute or relative terms.\ndf %\u0026gt;% select(date, regions, tamponi, nuovi_attualmente_positivi) %\u0026gt;% group_by(regions) %\u0026gt;% arrange(date) %\u0026gt;% mutate(new_tests = tamponi - lag(tamponi, 1)) %\u0026gt;% ungroup() %\u0026gt;% select(-tamponi) %\u0026gt;% gather(key = \u0026#39;variable\u0026#39;, value = \u0026#39;value\u0026#39;, -regions, -date) %\u0026gt;% mutate(variable = ifelse(variable == \u0026#39;new_tests\u0026#39;, \u0026#39;New tests\u0026#39;, \u0026#39;New positive cases\u0026#39;)) %\u0026gt;% ggplot(., aes(x = date, y = value/1000, colour = variable)) + geom_point() + geom_line() + facet_wrap(~regions, scales = \u0026#39;free_y\u0026#39;) + theme(axis.text.x = element_text(angle = 90)) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;thousands\u0026#39;, colour = \u0026#39;\u0026#39;) + scale_colour_brewer(palette = \u0026#39;Set1\u0026#39;) ## Warning: Removed 21 rows containing missing values (geom_point). ## Warning: Removed 1 row(s) containing missing values (geom_path). df %\u0026gt;% mutate(positive_share = totale_attualmente_positivi/tamponi) %\u0026gt;% ggplot(., aes(x = date, y = positive_share*100, colour = \u0026#39;Positive cases as a share of total tested\u0026#39;)) + geom_point() + geom_line() + facet_wrap(~regions, scales = \u0026#39;free_y\u0026#39;) + theme(axis.text.x = element_text(angle = 90)) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;%\u0026#39;) ## Warning: Removed 15 rows containing missing values (geom_point). But it is still difficult to make conclusions because of selection issues. And increase in the share of positive cases in all tested may either of the two:\n more people are getting infected tests are given to those most likely to have the virus.   Death rate and new death cases df %\u0026gt;% mutate(death_rate = deceduti/totale_casi) %\u0026gt;% ggplot(., aes(x = date, y = death_rate*100, colour = \u0026#39;Death rate\u0026#39;)) + geom_point() + geom_line() + facet_wrap(~regions, scales = \u0026#39;fixed\u0026#39;) + theme(axis.text.x = element_text(angle = 90)) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;%\u0026#39;, colour = \u0026#39;\u0026#39;) ## Warning: Removed 73 rows containing missing values (geom_point). ## Warning: Removed 1 row(s) containing missing values (geom_path). df %\u0026gt;% group_by(regions) %\u0026gt;% arrange(date) %\u0026gt;% mutate(new_death = deceduti - lag(deceduti, 1)) %\u0026gt;% ungroup() %\u0026gt;% ggplot(., aes(x = date, y = new_death, fill = regions)) + geom_bar(position = \u0026#39;stack\u0026#39;, stat = \u0026#39;identity\u0026#39;) + scale_fill_manual(values = myColourPalette) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;New death cases\u0026#39;, fill = \u0026#39;Regions\u0026#39;) ## Warning: Removed 21 rows containing missing values (position_stack). ggsave(filename = \u0026#39;featured.png\u0026#39;) ## Saving 7 x 5 in image ## Warning: Removed 21 rows containing missing values (position_stack).  Timeline on a map ndays \u0026lt;- min(df$date) %--% max(df$date) / ddays(1) + 1 map \u0026lt;- df_sf %\u0026gt;% mutate(death_rate = deceduti/totale_casi) %\u0026gt;% ggplot(.) + geom_sf(data = shape, fill = NA) + geom_sf(aes(color = death_rate*100, size = death_rate*100)) + theme_void() + labs(title = \u0026#39;Date: {frame_time}\u0026#39;, colour = \u0026#39;Death rate\u0026#39;, size = \u0026#39;Death rate\u0026#39;) + scale_colour_distiller(palette = \u0026#39;Spectral\u0026#39;) + transition_time(date) #animate(map, nframes = 30, renderer = gifski_renderer(\u0026#39;map_png/map_anim.gif\u0026#39;)) animate(map, nframes = ndays, fps = 30, renderer = file_renderer(\u0026#39;map_png/\u0026#39;, overwrite = TRUE)) gifski::gifski(list.files(\u0026#39;map_png/\u0026#39;, full.names = \u0026#39;TRUE\u0026#39;), gif_file = \u0026#39;map_anim.gif\u0026#39;)   ","date":1585144808,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585169180,"objectID":"76792a656d9cc269cbaecce4eed556a8","permalink":"https://nurfatimaj.com/project/covid-19/","publishdate":"2020-03-25T15:00:08+01:00","relpermalink":"/project/covid-19/","section":"project","summary":" ","tags":[],"title":"Covid-19 in Italy in graphs","type":"project"},{"authors":null,"categories":["tech fun"],"content":"It all started with my supervisor filling in a data request form for the joint project and \u0026ldquo;complaining\u0026rdquo; that his RAs still don\u0026rsquo;t have their websites. I thought that I\u0026rsquo;d need one in a year\u0026rsquo;s time anyway, so why not do it now?\nMaybe you can guess from the previous post, but I am a person who just doesn\u0026rsquo;t like unnecessarily duplicating work. Therefore, I wanted my website generate my CV document since they share most of the information.\nAt this point, you probably think that I have too much time to waste. You are probably right. But I had already started this journey and I needed to bring it to a satisfactory finish. And I also like to think that it benefits future me by freeing up a little bit of time each year from needing to update profile twice.\nThe general idea is simple: to have all my CV entries in a kind of database and let the algorithms put them both to website and CV document. We are living in an age of artificial intelligence after all. Should be easy peasy!\nWordPress My first attempt was WordPress because of rich functionality. In particular, it already has some special CV plugins that save my information into a specific database and build CV pages using this database. However, making it output same information into a beautiful pdf has proven to be very difficult to do using open source solutions. In a hindsight, I must say that WordPress is also an overkill for such a simple personal page project.\nHugo Luckily, my boyfriend knows a lot of things and has introduced me to the world of Hugo. It is a simple, free, static-site generator with the support of data-driven content. The financial benefit of Hugo is that I can host it for free on GitHub; thus, only paying for a custom domain name. Personally, it feels that comparing Hugo to WordPress is like comparing LaTeX to Word.\nWhile, it may be less user-friendly than WordPress, it would have only taken me couple of hours to have my site up and running. However, I needed a functionality not offered by ready-to-use themes. So, I needed some tweaking.\nhugo-resume First, I was working with hugo-resume theme. It had the most important feature I needed: data-driven content.\nBut how do I get my CV as a pdf file? I had an idea. I could generate a second page that looks exactly like I want my CV document to look and simply print it to pdf in the browser. To make it a bit more explicit, I have also included a button in the home page that opens the printer dialogue attempting to print the CV page. All print dialogues should have \u0026lsquo;Save as PDF\u0026rsquo; option.\nIt worked, but it wasn\u0026rsquo;t a perfect solution.\n Showing a print window with a possible choice of saving a file as pdf is making too many suppositions on behalf of the user. I would still prefer having a link to a pdf file. I have made so many tweaks to the theme that it basically became a completely new theme. Which meant that there are many places, where a potential bug could be sitting. Which meant maintenance would require quite a bit of my time. Related to the previous one, I did not like the visual appeal of the original theme and had to add style changes as well. This of course increases probability of having a buggy code.  hugo-academic It is porbably the most popular theme on Hugo. And you can tell it from the rigour of the theme\u0026rsquo;s documentation. The developers have thought about many features. Except for the possibility of sending the website data into a beautiful pdf.\nTweaking this website is easy and difficult at the same time.\n Why is it difficult? Precisely because of its flexbility, the structure of the theme is not the easiest to understand. So, even if I knew exactly what I want to tweak, it took me some time to understand where should I apply those tweaks. Beyond beginner knowledge of Hugo is required. Why is it easy? Because the files are organized into nice small chunks divided by functionality and there is a very good documentation with a step-by-step guide of creating your own \u0026ldquo;widget\u0026rdquo;. Unlike the previous theme, this one has been adapted to allow different user tastes for looks. I only needed to add a bit of code that converts CV to pdf file.  After having had some experience with Hugo templating, creating a new page with information for CV did not take much time. But how do I get a link to a pdf file instead of getting a print dialogue? Initially, I thought of printing to pdf before sending the data to the webpage. That is, I build website on my computer, send the generated page to printer, save it as pdf, add the link to this pdf on the home and send the website to GitHub. To automate this process, I used chrome command line interface, the worst solution I have tried (except for the idea of two separate files). The thing with chrome CLI is that it doesn\u0026rsquo;t work properly with chrome running in the background (and even without). The resulting pdf file always had different levels of white overlay.\nI was very unhappy about this. And was thinking \u0026ldquo;Oh, if only there was a way to let LaTeX generate this poor pdf!\u0026quot;. But the problem with this is that I don\u0026rsquo;t know how to access website information from within LaTeX.\nThe solution came to me the same way as the chemical table came to Mendeleev - in the dream. The idea - and current solution - is to have an auxiliary html page for CV with LaTeX code. For example, instead of \u0026lt;h2\u0026gt;Contact Information\u0026lt;/h2\u0026gt; for section names I put \\section*{Contact Information} into the template. Then, I run Python script to extract the LaTeX codes and supply them to pdflatex to generate a good-looking pdf. This is the best solution as it allows me\n to have my CV information in one place that fuels both webpage and CV document, and to control the look of CV document via latex commands.  Final words I surely hope that this is the end of this saga and I will not have another crazy idea to add to the CV ðŸ˜… Apart from that, I feel this is the ideal solution to my initial request!\n","date":1571586300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571586572,"objectID":"e9bd2643a471e82b350887986ad4745d","permalink":"https://nurfatimaj.com/post/tech-fun/personal-page-quest/","publishdate":"2019-10-20T17:45:00+02:00","relpermalink":"/post/tech-fun/personal-page-quest/","section":"post","summary":"It all started with my supervisor filling in a data request form for the joint project and \u0026ldquo;complaining\u0026rdquo; that his RAs still don\u0026rsquo;t have their websites. I thought that I\u0026rsquo;d need one in a year\u0026rsquo;s time anyway, so why not do it now?\n","tags":["data-driven website","automatic CV","Hugo","latex"],"title":"[Not] easy personal website","type":"post"},{"authors":null,"categories":["tech fun"],"content":"At the beginning of PhD I used to have at most one project at a time. Keeping track of things was easy. Along the way, I accumulated a portfolio of projects. For each of them I write tons of codes (partly because my programmer sister has taught me that having one big file with everything is bad). One day I found myself looking at my own code of three months earlier and spending half a day to retrace its logic. There were dozens of errors before I was able to run everything smoothly without forgetting some annoying little detail.\nThen it became clear to me that I need to keep a proper documentation. Hopefully, it is also helpful when one day the papers get published. Be it couple of files, I could have done it all manually. Or maybe not\u0026hellip; Just the thought that I would need to keep track of twice as many documents sends me to shiver. Search for some ready solution did not yield much results: either my target programming languages are too \u0026ldquo;simple\u0026rdquo;, or the output is not what I was looking for.\nAfter a little thinking it struck me that automatic documentation is nothing more than a text parsing exercise. So I set out to write my own script. It searches for all new or recently modified code files in my project folder, parses the text and pastes it to a markdown file in my documentation subfolder. In this project I rely a lot on git version control and remote repository that keeps all important code files.\nTechnical bits Code structure The benefit of having an automatic documentation generation script is that I can keep all documentation-relevant information inside the code file. Then keeping documentation up-to-date is only a matter of updating comments inside the code file.\nSpecial comment Not all comments are meant to be inside a documentation file. Detecting the relevant code blocks is solved by including a special comment block (#' or *' for Stata) which I have borrowed from R. So all lines that start with the special sign will be part of the documentation file.\nParsing and outputing After detecting the special comment block, the script is ridiculously simple. Basic idea is to strip away the special comment sign and simply copy the rest of the text into a markdown file. Of course, for this to work, the documentation comment block should follow a simple markdown syntax.\nMy documentation script applies additional rules to certain lines, which, again borrowing from R, are identified with special tags (for example, @title for document title). These rules work by identifying if a line starts with one of the special tags, stripping the tag away and outputing the rest of the text in a desired way. For example, when a line starts with @title, I insert to the title text hyperlink to the actual code file. The linking is possible because I am using git version control with a remote repository.\nTo a more technically inclined reader, all that my script does here is continuously apply sed to the extracted lines and echo the output into a markdown file.\nInput The script works both with single code file and with a project folder that contains the code files. That is, it is really easy to mass generate documentation files.\nHowever, not all the codes in the folder need documentation; for example, a script from workshop I use as a reference. My script relies on git version control in deciding which files are relevant: if it is controlled by git, then it is worth a documentation.\nPublishing Since I am using git and remote repository, I am also using wiki submodule to publish the documentation files. While GitHub requires public repository for wiki functionality to work, Bitbucket allows it on private repositories too. The path to the documentation files inside wiki follows is similar to the code file path inside project directory.\nIf I have time in the future, \u0026hellip; \u0026hellip; I would like to make the script a bit more flexible. Currently, my script parses and outputs documentation information line by line. Therefore, it can neither reorder sections nor augment with information found later under the same section heading. The output will look exactly the same way as the comment block. So, a nice extension would have been to extract all section-relevant data into a separate array and then output everything together. This would allow special comments organized like this\n#' @section1 #' comment 1 #' #' @section2 #' comment 1 #' #' @section1 #' comment 2 to appear correctly in the documentation file: two comments under section 1 and one comment under section 2.\nSummary Now, I can just click on respective wiki post and see all relevant information: usage, argument types and accepted values, description, input and output files. No need to traverse the code to make sense of it. Just work on the actual task.\n","date":1566038022,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571275533,"objectID":"1c238270ce3f9bd77542190e87b2151b","permalink":"https://nurfatimaj.com/post/tech-fun/automatic-documentation/","publishdate":"2019-08-22T20:00:00+06:00","relpermalink":"/post/tech-fun/automatic-documentation/","section":"post","summary":"At the beginning of PhD I used to have at most one project at a time. Keeping track of things was easy. Along the way, I accumulated a portfolio of projects. For each of them I write tons of codes (partly because my programmer sister has taught me that having one big file with everything is bad). One day I found myself looking at my own code of three months earlier and spending half a day to retrace its logic. There were dozens of errors before I was able to run everything smoothly without forgetting some annoying little detail.\n","tags":["software","documentation","git","bash script","markdown","text parsing"],"title":"Automatic documentation","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571299166,"objectID":"07e82b29ab4ada67b89956e1eb6ed8e1","permalink":"https://nurfatimaj.com/print/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/print/","section":"","summary":"","tags":null,"title":"CV","type":"widget_page"},{"authors":["Nurfatima Jandarova","Andrea Ichino","Aldo Rustichini","Johanna Reuter","Giulio Zanella"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571304500,"objectID":"ece28276351dd5408568671f8a778271","permalink":"https://nurfatimaj.com/publication/work-in-progress/uni-sorting-genetics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/work-in-progress/uni-sorting-genetics/","section":"publication","summary":"In the UK the number of students with a university degree has increased considerably between 1950 and 1970. Out of 100 high-school students, only about 3 went to university at the beginning of this period while about 8 did so in 1970 and about 19 in 1990. We know that, with few exceptions, only rich children had the possibility to attend college in 1950, independently of their skills. What we do not know is the answer to this question: did the expansion of enrollment after 1950 succeed in giving poor but smart children the opportunity to access a university? The objective of this research project is to answer this question. It is an important question because if, the answer is no, politicians need to think of better policies to achieve the goal of improving the opportunities of smart but poor children.","tags":null,"title":"Genetic Endowment and Sorting into University Education","type":"publication"},{"authors":["Nurfatima Jandarova","Johanna Reuter"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571304500,"objectID":"2369091e170bf255ee956ebcbb898e41","permalink":"https://nurfatimaj.com/publication/work-in-progress/measurement-edu-uk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/work-in-progress/measurement-edu-uk/","section":"publication","summary":"","tags":null,"title":"Improving UK university education rate statistic using multiple imputation","type":"publication"},{"authors":["Nurfatima Jandarova"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571275533,"objectID":"2195c6d4d67be36d5d70fbcf0e75c660","permalink":"https://nurfatimaj.com/publication/work-in-progress/edu-unemp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/work-in-progress/edu-unemp/","section":"publication","summary":"This project studies the effect of labour market shocks on educational choices and whether these educational choices translate into more favourable outcomes later in life. For causal identification I exploit local employment shocks at the time of finishing compulsory schooling using the rich dataset on the population of university students in the UK from 1972 to 1993.","tags":null,"title":"Unemployment and educational choices","type":"publication"}]