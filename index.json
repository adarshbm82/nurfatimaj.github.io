[{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"https://labuve.github.io/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\n Don’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution.    ","permalink":"https://labuve.github.io/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"https://labuve.github.io/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"https://labuve.github.io/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":["tech fun"],"contents":"At the beginning of PhD I used to have at most one project at a time. Keeping track of things was easy. Along the way, I accumulated a portfolio of projects. For each of them I write tons of codes (partly because my programmer sister has taught me that having one big file with everything is bad). One day I found myself looking at my own code of three months earlier and spending half a day to retrace its logic. There were dozens of errors before I was able to run everything smoothly without forgetting some annoying little detail.\nThen it became clear to me that I need to keep a proper documentation. Hopefully, it is also helpful when one day the papers get published. Be it couple of files, I could have done it all manually. Or maybe not\u0026hellip; Just the thought that I would need to keep track of twice as many documents sends me to shiver. Search for some ready solution did not yield much results: either my target programming languages are too \u0026ldquo;simple\u0026rdquo;, or the output is not what I was looking for.\nAfter a little thinking it struck me that automatic documentation is nothing more than a text parsing exercise. So I set out to write my own script. It searches for all new or recently modified code files in my project folder, parses the text and pastes it to a markdown file in my documentation subfolder. In this project I rely a lot on git version control and remote repository that keeps all important code files.\nTechnical bits Code structure The benefit of having an automatic documentation generation script is that I can keep all documentation-relevant information inside the code file. Then keeping documentation up-to-date is only a matter of updating comments inside the code file.\nSpecial comment Not all comments are meant to be inside a documentation file. Detecting the relevant code blocks is solved by including a special comment block (#' or *' for Stata) which I have borrowed from R. So all lines that start with the special sign will be part of the documentation file.\nParsing and outputing After detecting the special comment block, the script is ridiculously simple. Basic idea is to strip away the special comment sign and simply copy the rest of the text into a markdown file. Of course, for this to work, the documentation comment block should follow a simple markdown syntax.\nMy documentation script applies additional rules to certain lines, which, again borrowing from R, are identified with special tags (for example, @title for document title). These rules work by identifying if a line starts with one of the special tags, stripping the tag away and outputing the rest of the text in a desired way. For example, when a line starts with @title, I insert to the title text hyperlink to the actual code file. The linking is possible because I am using git version control with a remote repository.\nTo a more technically inclined reader, all that my script does here is continuously apply sed to the extracted lines and echo the output into a markdown file.\nInput The script works both with single code file and with a project folder that contains the code files. That is, it is really easy to mass generate documentation files.\nHowever, not all the codes in the folder need documentation; for example, a script from workshop I use as a reference. My script relies on git version control in deciding which files are relevant: if it is controlled by git, then it is worth a documentation.\nPublishing Since I am using git and remote repository, I am also using wiki submodule to publish the documentation files. While GitHub requires public repository for wiki functionality to work, Bitbucket allows it on private repositories too. The path to the documentation files inside wiki follows is similar to the code file path inside project directory.\nIf I have time in the future, \u0026hellip; \u0026hellip; I would like to make the script a bit more flexible. Currently, my script parses and outputs documentation information line by line. Therefore, it can neither reorder sections nor augment with information found later under the same section heading. The output will look exactly the same way as the comment block. So, a nice extension would have been to extract all section-relevant data into a separate array and then output everything together. This would allow special comments organized like this\n#' @section1 #' comment 1 #' #' @section2 #' comment 1 #' #' @section1 #' comment 2  to appear correctly in the documentation file: two comments under section 1 and one comment under section 2.\nSummary Now, I can just click on respective wiki post and see all relevant information: usage, argument types and accepted values, description, input and output files. No need to traverse the code to make sense of it. Just work on the actual task.\n","permalink":"https://labuve.github.io/blog/tech-fun/automatic-documentation/","tags":["software","documentation","git","bash script","markdown","text parsing"],"title":"Automatic documentation"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip; is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"https://labuve.github.io/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"https://labuve.github.io/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace. Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility\n","permalink":"https://labuve.github.io/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":" This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;JSON\u0026quot;]  Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026quot;contents\u0026quot;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026quot;tags\u0026quot;:{{ .Params.tags | jsonify }}{{end}}, \u0026quot;categories\u0026quot; : {{ .Params.categories | jsonify }}, ...  Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026quot;title\u0026quot;, \u0026quot;contents\u0026quot;, \u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot; ]  ","permalink":"https://labuve.github.io/search/","tags":null,"title":"Search Results"}]